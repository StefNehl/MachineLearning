%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chair of Cyber-Physical-Systems
% Univ.-Prof. Dr. Elmar Rueckert
% Montanuniversität Leoben, Austria
% Latest Update: Feb. 2022
%
% Disclaimer: The materials and source code are for personal use only. The material is intended for educational purposes only. Reproduction of the material for any purposes other than what is intended is prohibited. The content is to be used for educational and non-commercial purposes only and is not to be changed, altered, or used for any commercial endeavor without the express written permission of Professor Rueckert. 
% 
% Original Version by Frits Wenneker, 28/2/17,  License: CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[10pt, a4paper, twocolumn]{article} % 10pt font size (11 and 12 also possible), A4 paper (letterpaper for US letter) and two column layout (remove for one column)

\input{structure.tex} % Specifies the document structure and loads requires packages

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\title{Assignment V: Gaussian Process Regression} % The article title

\author{
	\coursetitle{Exercises in Machine Learning (190.013), SS2022}
	\authorstyle{Stefan Nehl\textsuperscript{1}} % Authors
	\newline\newline % Space before institutions
	\textsuperscript{1}\textit{stefan-christopher.nehl@stud.unileoben.ac.at, MNr: 00935188}, \institution{Montanuniversität Leoben, Austria}\\ % Institution 1
	\newline\submissiondate{\today} % Add a date here
}

% Example of a one line author/institution relationship
%\author{\newauthor{John Marston} \newinstitution{Universidad Nacional Autónoma de México, Mexico City, Mexico}}


%----------------------------------------------------------------------------------------

\begin{document}
\input{python_code.tex} % To print Python code

\maketitle % Print the title

\thispagestyle{firstpage} % Apply the page style for the first page (no headers and footers)

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\lettrineabstract{In the fifth assignment, I had to describe the Guassian Process Regression and implement it with the library GPy. Furthermore, I had to test different kernel implementations and hyper parameters and verify and compare the results with the results of the last assignment.}

%----------------------------------------------------------------------------------------
%	REPORT CONTENTS
%----------------------------------------------------------------------------------------

\section{Gaussian Process Regression}
Gaussian processes are a class of nonparametric models for machine learning. They are commonly used for modeling spatial and time series data. \citep{gaussianProcessRegression} The \textit{Gaussian Process Regression} uses the 
\textit{Multivariate Conditional Distribution}. 

\[
f(\boldsymbol{x'}|\boldsymbol{\mu},\boldsymbol{\Sigma}) = \frac{1}{\sqrt{(2\pi)\textsuperscript{k}\vert\boldsymbol{\Sigma}\vert}
}
\:exp\textsuperscript{$-\frac{1}{2}(\boldsymbol{x'}-\boldsymbol{\mu})\textsuperscript{T}\boldsymbol{\Sigma}\textsuperscript{-1}(\boldsymbol{x'}-\boldsymbol{\mu})$}
\]With $\boldsymbol{x'}$= $\lbrace$x'$_{1}$, ..., $x'_{k}\rbrace$, $\boldsymbol{\Sigma}$ the covariance matrix and $\boldsymbol{|\Sigma|}$ the determinant of the covariance matrix. The covariance is determined by the covariance function of the kernel and has to be positive definite. \citep{bookMachineLearning}
I tried three different kernels for the \textit{Gaussian Process Regression}. The \textit{Linear Kernel}, the \textit{RBF}, \textit{Radial Basis Function} and the \textit{Matern 52}. 

\subsection{Linear Kernel}
The \textit{Linear Kernel} is based on linear classification. This classification is based on the linear combination of the characteristics. The decision function can be described with: 
\[
d(\boldsymbol{x}) = \boldsymbol{w}^{T}\phi(\boldsymbol{x}) + b
\]
where $\boldsymbol{w}$ is the weight vector, b a biased value and $\phi(\boldsymbol{x})$ a higher dimensional vector of \textbf{x}. If $\boldsymbol{w}$ is a linear combination of training data $\boldsymbol{w}$ can be calculated with: 
\[
\boldsymbol{w} = \sum_{i=1}^{l}\boldsymbol{\alpha}_i\phi(\boldsymbol{x_i})
\]
for some $\boldsymbol{\alpha}$ $\in$ $\textbf{R}^{1}$
The kernel function can be calculated with 
\[
K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) = \phi(\boldsymbol{x}_{i}))^{T}\phi(\boldsymbol{x}_{j}))
\]
\citep{linearKernel}
The linear kernel is good for data with a lot of features. That's because mapping the data to a higher dimensional space does not really improve the performance. 
\citep{linearKernelRecommended}
The implementation in the \textit{GPy} library was the following: 
\[
K(x,y) = \sum_{i=1}^{D} \sigma^2_i x_iy_i
\]
Where $D$ defines the dimension and $\sigma^2_i$ the variance for each dimension.

\subsection{RBF}
The \textit{Radial Basis Function} is one of the most used kernels. It's similar to the \textit{Gaussian distribution}. The kernel calculates the similarity or how close two points are to each other. 
\[
K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) = \:epx(-\frac{||\boldsymbol{x}_{i} - \boldsymbol{x}_{j}||}{2\sigma^{2}})
\]
Where $||\boldsymbol{x}_{i} - \boldsymbol{x}_{j}||$ is the euclidean ($L_{2}$-Norm) and $\sigma$ the variance and the hyper parameter.
\citep{rbfKernel}
\[
K(r) = \sigma^2 \exp \bigg(- \frac{1}{2} r^2 \bigg)
\]
Implementation in the \textit{GPy} library where $r = |\boldsymbol{x}_{i} - \boldsymbol{x}_{j}|$ and $\sigma^2$ the variance.

\subsection{Matern 52}
The \textit{Matern 52} is a generalization of the \textit{RBF} kernel. 
\[
K(r) = (1 + \frac{\sqrt{5}r}{l} + \frac{5r^{2}}{3l^{2}})\:exp(-\frac{\sqrt{5}r}{l})
\]
Where $r = |\boldsymbol{x}_{i} - \boldsymbol{x}_{j}|$ and $l$ a positive parameter.\citep{maternKernel}
However, if I take a look in the code of the \textit{GPy} implementation. It looks a little bit different.
\[
K(r) = \sigma^2 (1 + \sqrt{5} r + \frac{5}{3} r^2) \exp(- \sqrt{5} r)
\]
It looks like, the positive parameter $l$ is set to 1. As additional hyper parameter the variance, $\sigma^{2}$, was introduced. 


\subsection{Hyper-Parameters}
The \textit{GPy} library has for every kernel a variance parameter. This parameter is a hyper parameter to adjust improve the prediction result. If I set the parameter to a lower value the values for the learning are in a smaller gap. If I increase $\sigma^2$ the gab increases. So if the variance of my data is high, an increased value for the parameter variance makes sense.

\subsection{Implementation}
For the implementation of the \textit{Gaussian Process Regression} i created a the class \textit{GaussianProcess} which derived from the class \textit{Regression} and reused the functions, \textit{importData}, \textit{generateTrainingSubset}, \textit{createFeatureVector}, \textit{testModel}, \textit{computMeanOfError}, \textit{getMeanError}, \textit{plotError} and \textit{plotHeatMap} from the last assignment about \textit{Ridge Regression}. I implemented than the function \textit{computeGaussianProcessRegression} which takes the parameters \textit{kernelSetting} and \textit{variance}. The parameter \textit{kernelSetting} is an enum with the following values: 
\begin{table}[htbp]
    \label{tab:kernelSettings}
	\caption{Values for the kernel settings}
	\centering
	\begin{tabular}{llr}
		\cmidrule(r){1-2}
		Value & String \\
		\midrule
		LinearKernel & Linear Kernel \\
		RBFWithGpu & RBF with GPU \\
		RBF & RBF \\				
		Matern52 & Matern 52 \\
		\bottomrule
	\end{tabular}
\end{table}
I checked those values with an if and set the kernel to the corresponding value.
The kernel function, \textit{GPY.kern.KernelName} got 2 more parameters. One was the dimension of the data and the other one the variance, which is the hyper parameter. The dimension was set to two and for the variance I tried different values. After the kernel I created the model with the function \textit{GPY.models.GPRegression} which takes the x and y values and the kernel as a parameter. The y values where normalized to have a mean of zero in the data and I used only a subset of the training data to overcome the performance issues. The size of the subset can be set with the parameter \textit{trainStep} which has the same implementation from assignment 4. After the model creation I optimized the model with the \textit{model.optimize} function. This function takes the max iterations as a parameter. I set this value to 1000. After the optimization I used the \textit{model.predict} function and passed the y test data to the trained model. With the returned y values and the y test data from our data set, I calculated the error and the mean of the error. The last step was the plotting of the descending error, the heat map and compared the error with the error of the\textit{Ridge Regression}.

\subsection{Result}
First I tested which kernel performs the best. For this I created a test with the train steps of 20 and a hyper parameter of 1 and tested all three kernels and compared the error with the error of the \textit{Ridge Regression}. Figure 1 displays the different values. (I had an issue in the last assignment with the \textit{Ridge Regression}. This bug is now solved.) 
\begin{figure}[htbp] %or use htbp to place it inside the text blocks
  \centering
  \includegraphics[width=\columnwidth]{pics/TrainStep20_kernel_test.png}
  \caption{Error with different kernels and variance:1}
  \label{fig:fibonacciPlot}
\end{figure}
Figure 1 shows, that in this case the \textit{Ridge Regression} performs better than the \textit{Gaussian Processes }. From the \textit{Gaussian Processes} the \textit{Linear Kernel} performs better than the \textit{Matern 52} and the \textit{RBF}. Similar picture is also displayed in Figure 2 where I repeated the test, but with a variance of two.
\begin{figure}[htbp] %or use htbp to place it inside the text blocks
  \centering
  \includegraphics[width=\columnwidth]{pics/TrainStep20_kernel_test_v2.png}
  \caption{Error with different kernels and variance:2}
  \label{fig:fibonacciPlot}
\end{figure}
Also the optimization process took some time. Table 2 displays the optimization times from the different kernels. 
\begin{table}[htbp]
    \label{tab:kernelSettings}
	\caption{Optimization time for the different kernels}
	\centering
	\begin{tabular}{llr}
		\cmidrule(r){1-2}
		Kernel & Opt Time \\
		\midrule
		RBF & 1m 13\\	
		Matern 52 & 1m 29s\\				
		Linear Kernel & 1m 4s\\
		\bottomrule
	\end{tabular}
\end{table}
The times are not exact and are depending on the pc and if there are any other programs running in the background.
Next I tested the different kernels with 4 different values for the variance. The values were 0.1, 0.5, 1 and 2. Figure 3 shows the result of this test. 
\begin{figure}[htbp] %or use htbp to place it inside the text blocks
  \centering
  \includegraphics[width=\columnwidth]{pics/TrainStep20_Vtest.png}
  \caption{Error with different kernels and variance:0.1, 0.5, 1, 2}
  \label{fig:fibonacciPlot}
\end{figure}
The result displayed in Figure 3 shows, that with a high amount of optimization iterations the variance effect is small or not even visible. I repeated the test without any optimization. Figure 4 shows that without optimization the mean error decreases with a lower variance for this dataset. This behaviour is also displayed in Figure 5 where I tested the \textit{Matern 52} kernel with the different settings. 
\begin{figure}[htbp] %or use htbp to place it inside the text blocks
  \centering
  \includegraphics[width=\columnwidth]{pics/TrainStep20_Vtest_withoutOpt.png}
  \caption{Error with different kernels and variance:0.1, 0.5, 1, 2 and without optimization}
  \label{fig:fibonacciPlot}
\end{figure}
\begin{figure}[htbp] %or use htbp to place it inside the text blocks
  \centering
  \includegraphics[width=\columnwidth]{pics/TrainStep20_Vtest_matern_withoutOpt.png}
  \caption{Error with Matern 52 kernel and variance:0.1, 0.5, 1, 2 and without optimization}
  \label{fig:fibonacciPlot}
\end{figure}
Table 3 displays all results. Without the optimization there is no time measured for the optimization time. 
\begin{table}[htbp]
    \label{tab:kernelSettings}
	\caption{Kernel Results}
	\centering
	\begin{tabular}{ccccc}
		\cmidrule(r){1-5}
		Kernel & Variance & Optimized & Error Mean & Opt Time \\
		\midrule
		RBF & 0.1 & True & 1.7511 & 1m 16s\\	
		RBF & 0.5 & True & 1.7511 & 1m 06s\\					
		RBF & 1.0 & True & 1.7511 & 1m 13s\\
		RBF & 2.0 & True & 1.7511 & 1m 24s\\
		Matern 52 & 0.1 & True & 1.7520 & 1m 21s\\	
		Matern 52 & 0.5 & True & 1.7520 & 1m 24s\\					
		Matern 52 & 1.0 & True & 1.7520 & 1m 29s\\
		Matern 52 & 2.0 & True & 1.7520 & 1m 40s\\
		LK & 0.1 & True & 1.6533 & 50s\\	
		LK & 0.5 & True & 1.6533 & 1m 02s\\					
		LK & 1.0 & True & 1.6533 & 1m 04s\\
		LK & 2.0 & True & 1.6533 & 1m 09s\\
		RBF & 0.1 & False & 1.6925 & -\\	
		RBF & 0.5 & False & 1.7309 & -\\					
		RBF & 1.0 & False & 1.7368 & -\\
		RBF & 2.0 & False & 1.7401 & -\\
		Matern 52 & 0.1 & False & 1.6906 & -\\	
		Matern 52 & 0.5 & False & 1.7307 & -\\					
		Matern 52 & 1.0 & False & 1.7373 & -\\
		Matern 52 & 2.0 & False & 1.7413 & -\\
		LK & 0.1 & False & 1.6532 & -\\	
		LK & 0.5 & False & 1.6533 & -\\					
		LK & 1.0 & False & 1.6533 & -\\
		LK & 2.0 & False & 1.6533 & -\\
		\bottomrule
	\end{tabular}
\end{table}
For the last test I used the \textit{Linear Kernel} and compared the results with the \textit{Ridge Regression}. 
For this I set the optimization value to 1000, the variance to 1 and the lambda value of the \textit{Ridge Regression} to 1. I compared the error and the heat map of those regression models. Figure 6 displays the result. 
\begin{figure}[htbp] %or use htbp to place it inside the text blocks
  \centering
  \includegraphics[width=\columnwidth]{pics/TrainStep20_LK_RR.png}
  \caption{Error with Ridge Regression and Linear Kernel}
  \label{fig:fibonacciPlot}
\end{figure}
The mean of the \textit{Ridge Regression} is smaller. However, the \textit{Linear Kernel} performs better in some areas. This is also visible in Figure 7 and 8 which displays the heat map of both. The \textit{Linear Kernel} performs better in the center of the given longitude and latitude dataset and the \textit{Ridge Regression} better and the right side of the dataset. 
\begin{figure}[htbp] %or use htbp to place it inside the text blocks
  \centering
  \includegraphics[width=\columnwidth]{pics/TrainStep20_HM_RR.png}
  \caption{Heatmap Ridge Regression}
  \label{fig:fibonacciPlot}
\end{figure}
\begin{figure}[htbp] %or use htbp to place it inside the text blocks
  \centering
  \includegraphics[width=\columnwidth]{pics/TrainStep20_HM_LK.png}
  \caption{Heatmap Linear Kernel}
  \label{fig:fibonacciPlot}
\end{figure}
\subsection{Conclusion}
The implementation with the \textit{GPy} was easy and with the help of the tutorials good doable. The optimization of the kernel has a big performance impact on the mean and does not always provide better results. I mixture between smaller optimization values and a good picked hyper parameter can improve the overall results. However, 
in all the cases the \textit{Ridge Regression} had a better mean than the \textit{Gaussian Processes}. 



%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\printbibliography[title={Bibliography}] % Print the bibliography, section title in curly brackets

%----------------------------------------------------------------------------------------


\onecolumn\section*{APPENDIX}
\onecolumn\lstinputlisting{../TaskGaussianProcess.py}
\onecolumn\lstinputlisting{../../Modules/GaussianProcess.py}
\onecolumn\lstinputlisting{../../Modules/inference.py}


\end{document}
