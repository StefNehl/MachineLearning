%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chair of Cyber-Physical-Systems
% Univ.-Prof. Dr. Elmar Rueckert
% Montanuniversität Leoben, Austria
% Latest Update: Feb. 2022
%
% Disclaimer: The materials and source code are for personal use only. The material is intended for educational purposes only. Reproduction of the material for any purposes other than what is intended is prohibited. The content is to be used for educational and non-commercial purposes only and is not to be changed, altered, or used for any commercial endeavor without the express written permission of Professor Rueckert. 
% 
% Original Version by Frits Wenneker, 28/2/17,  License: CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[10pt, a4paper, twocolumn]{article} % 10pt font size (11 and 12 also possible), A4 paper (letterpaper for US letter) and two column layout (remove for one column)

\input{structure.tex} % Specifies the document structure and loads requires packages

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\title{Assignment V: Gaussian Process Regression} % The article title

\author{
	\coursetitle{Exercises in Machine Learning (190.013), SS2022}
	\authorstyle{Stefan Nehl\textsuperscript{1}} % Authors
	\newline\newline % Space before institutions
	\textsuperscript{1}\textit{stefan-christopher.nehl@stud.unileoben.ac.at, MNr: 00935188}, \institution{Montanuniversität Leoben, Austria}\\ % Institution 1
	\newline\submissiondate{\today} % Add a date here
}

% Example of a one line author/institution relationship
%\author{\newauthor{John Marston} \newinstitution{Universidad Nacional Autónoma de México, Mexico City, Mexico}}


%----------------------------------------------------------------------------------------

\begin{document}
\input{python_code.tex} % To print Python code

\maketitle % Print the title

\thispagestyle{firstpage} % Apply the page style for the first page (no headers and footers)

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\lettrineabstract{In the fifth assignment, I had to describe the Guassian Process Regression and implement it with the library GPy. Furthermore, I had to test different kernel implementations and hyper parameters and verify and compare the results with the results of the last assignment.}

%----------------------------------------------------------------------------------------
%	REPORT CONTENTS
%----------------------------------------------------------------------------------------

\section{Gaussian Process Regression}
Gaussian processes are a class of nonparametric models for machine learning. They are commonly used for modeling spatial and time series data. \citep{gaussianProcessRegression} The \textit{Gaussian Process Regression} uses the 
\textit{Multivariate Conditional Distribution}. 

\[
f(\boldsymbol{x'}|\boldsymbol{\mu},\boldsymbol{\Sigma}) = \frac{1}{\sqrt{(2\pi)\textsuperscript{k}\vert\boldsymbol{\Sigma}\vert}
}
\:exp\textsuperscript{$-\frac{1}{2}(\boldsymbol{x'}-\boldsymbol{\mu})\textsuperscript{T}\boldsymbol{\Sigma}\textsuperscript{-1}(\boldsymbol{x'}-\boldsymbol{\mu})$}
\]With $\boldsymbol{x'}$= $\lbrace$x'$_{1}$, ..., $x'_{k}\rbrace$, $\boldsymbol{\Sigma}$ the covariance matrix and $\boldsymbol{|\Sigma|}$ the determinant of the covariance matrix. The covariance is determined by the covariance function of the kernel and has to be positive definite. \citep{bookMachineLearning}
I tried three different kernels for the \textit{Gaussian Process Regression}. The \textit{Linear Kernel}, the \textit{RBF}, \textit{Radial Basis Function} and the \textit{Matern 52}. 

\subsection{Linear Kernel}
The \textit{Linear Kernel} is based on linear classification. This classification is based on the linear combination of the characteristics. The decision function can be described with: 
\[
d(\boldsymbol{x}) = \boldsymbol{w}^{T}\phi(\boldsymbol{x}) + b
\]
where $\boldsymbol{w}$ is the weight vector, b a biased value and $\phi(\boldsymbol{x})$ a higher dimensional vector of \textbf{x}. If $\boldsymbol{w}$ is a linear combination of training data $\boldsymbol{w}$ can be calculated with: 
\[
\boldsymbol{w} = \sum_{i=1}^{l}\boldsymbol{\alpha}_i\phi(\boldsymbol{x_i})
\]
for some $\boldsymbol{\alpha}$ $\in$ $\textbf{R}^{1}$
The kernel function can be calculated with 
\[
K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) = \phi(\boldsymbol{x}_{i}))^{T}\phi(\boldsymbol{x}_{j}))
\]
\citep{linearKernel}
The linear kernel is good for data with a lot of features. That's because mapping the data to a higher dimensional space does not really improve the performance. 
\citep{linearKernelRecommended}
The implementation in the \textit{GPy} library was the following: 
\[
K(x,y) = \sum_{i=1}^{D} \sigma^2_i x_iy_i
\]
Where $D$ defines the dimension and $\sigma^2_i$ the variance for each dimension.

\subsection{RBF}
The \textit{Radial Basis Function} is one of the most used kernels. It's similar to the \textit{Gaussian distribution}. The kernel calculates the similarity or how close two points are to each other. 
\[
K(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) = \:epx(-\frac{||\boldsymbol{x}_{i} - \boldsymbol{x}_{j}||}{2\sigma^{2}})
\]
Where $||\boldsymbol{x}_{i} - \boldsymbol{x}_{j}||$ is the euclidean ($L_{2}$-Norm) and $\sigma$ the variance and the hyper parameter.
\citep{rbfKernel}
\[
K(r) = \sigma^2 \exp \bigg(- \frac{1}{2} r^2 \bigg)
\]
Implementation in the \textit{GPy} library where $r = |\boldsymbol{x}_{i} - \boldsymbol{x}_{j}|$ and $\sigma^2$ the variance.

\subsection{Matern 52}
The \textit{Matern 52} is a generalization of the \textit{RBF} kernel. 
\[
K(r) = (1 + \frac{\sqrt{5}r}{l} + \frac{5r^{2}}{3l^{2}})\:exp(-\frac{\sqrt{5}r}{l})
\]
Where $r = |\boldsymbol{x}_{i} - \boldsymbol{x}_{j}|$ and $l$ a positive parameter.\citep{maternKernel}
However, if I take a look in the code of the \textit{GPy} implementation. It looks a little bit different.
\[
K(r) = \sigma^2 (1 + \sqrt{5} r + \frac{5}{3} r^2) \exp(- \sqrt{5} r)
\]
It looks like, the positive parameter $l$ is set to 1. As additional hyper parameter the variance, $\sigma^{2}$, was introduced. 


\subsection{Hyper-Parameters}
The \textit{GPy} library has for every kernel a variance parameter. This parameter is a hyper parameter to adjust improve the prediction result. If I set the parameter to a lower value the values for the learning are in a smaller gap. If I increase $\sigma^2$ the gab increases. So if the variance of my data is high, an increased value for the parameter variance makes sense.

\subsection{Implementation}
For the implementation of the \textit{Gaussian Process Regression} i created a the class \textit{GaussianProcess} which derived from the class \textit{Regression} and reused the functions, \textit{importData}, \textit{generateTrainingSubset}, \textit{createFeatureVector}, \textit{testModel}, \textit{computMeanOfError}, \textit{getMeanError}, \textit{plotError} and \textit{plotHeatMap} from the last assignment about \textit{Ridge Regression}. I implemented than the function \textit{computeGaussianProcessRegression} which takes the parameters \textit{kernelSetting} and \textit{variance}. The parameter \textit{kernelSetting} is an enum with the following values: 
\begin{table}[htbp]
    \label{tab:kernelSettings}
	\caption{Values for the kernel settings}
	\centering
	\begin{tabular}{llr}
		\cmidrule(r){1-2}
		Value & String \\
		\midrule
		LinearKernel & Linear Kernel \\
		RBFWithGpu & RBF with GPU \\
		RBF & RBF \\				
		Matern52 & Matern 52 \\
		\bottomrule
	\end{tabular}
\end{table}

I checked those values with an if and set the kernel to the corresponding value.
The kernel function, \textit{GPY.kern.KernelName} got 2 more parameters. One was the dimension of the data and the other one the variance, which is the hyper parameter. The dimension was set to two and for the variance I tried different values. After the kernel I created the model with the function \textit{GPY.models.GPRegression} which takes the x and y values and the kernel as a parameter. The y values where normalized to have a mean of zero in the data and I used only a subset of the training data to overcome the performance issues. The size of the subset can be set with the parameter \textit{trainStep} which has the same implementation from assignment 4. After the model creation I optimized the model with the \textit{model.optimize} function. This function takes the max iterations as a parameter. I set this value to 1000. After the optimization I used the \textit{model.predict} function and passed the y test data to the trained model. With the returned y values and the y test data from our data set, I calculated the error and the mean of the error. The last step was the plotting of the descending error, the heat map and compared the error with the error of the\textit{Ridge Regression}.

\subsection{Result}
First I tested which kernel performs the best. For this I created a test with the train steps of 20 and a hyper parameter of 1 and tested all three kernels and compared the error with the error of the \textit{Ridge Regression}. Figure 1 displays the different values. 
\begin{figure}[htbp] %or use htbp to place it inside the text blocks
  \centering
  \includegraphics[width=\columnwidth]{pics/TrainStep20_V_1.png}
  \caption{Error with different kernels and variance:1}
  \label{fig:fibonacciPlot}
\end{figure}
Figure 1 shows, that in this case the \textit{Ridge Regression} performs better than the \textit{Gaussian Processes }. From the \textit{Gaussian Processes} the \textit{RBF} and the \textit{Matern 52} performs better than the \textit{Linear Kernel}. Also the time for the learning process took some time. Table 2 displays the learning time from the different kernels. 

\begin{table}[htbp]
    \label{tab:kernelSettings}
	\caption{Learning time for the different kernels}
	\centering
	\begin{tabular}{llr}
		\cmidrule(r){1-2}
		Kernel & Time \\
		\midrule
		RBF & 1m 53s\\	
		Matern 52 & 2m 13s\\				
		Linear Kernel & 1m 8s\\
		\bottomrule
	\end{tabular}
\end{table}
Because of the results and the performance I used the \textit{Matern 52} kernel for the next tests. I tested the \textit{Matern 52} with 4 different values for the variance. The values were 0.1, 0.5, 1 and 2. 
\subsection{Conclusion}



%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\printbibliography[title={Bibliography}] % Print the bibliography, section title in curly brackets

%----------------------------------------------------------------------------------------


\onecolumn\section*{APPENDIX}



\end{document}
